{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0243a72c",
   "metadata": {},
   "source": [
    "**Journal prediction on APS citations with Node2Vec**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f977e2eb",
   "metadata": {},
   "source": [
    "Goal is to predict the journal of 2013 papers using graph embeddings learned from the citation network.I will build Node2Vec embeddings on the full unlabeled graph, then train simple classifiers on 2010 to 2012 and evaluate on 2013. Node prediction problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9eaab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np, pickle, warnings\n",
    "from sklearn.linear_model import LogisticRegression # type: ignore\n",
    "from sklearn.ensemble import RandomForestClassifier # type: ignore\n",
    "from sklearn.svm import LinearSVC # type: ignore\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit # type: ignore\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, top_k_accuracy_score # type: ignore\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8dd7d",
   "metadata": {},
   "source": [
    "Read the Pajek network file with doi and year info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cb61752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|██████████| 56473/56473 [00:00<00:00, 474752.48it/s]\n",
      "Edges: 200592it [00:00, 1835820.81it/s]\n"
     ]
    }
   ],
   "source": [
    "def read_pajek_citations(filename):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        # Read header\n",
    "        first_line = file.readline()\n",
    "        n_nodes = int(first_line.split()[1])\n",
    "        \n",
    "        # Read nodes\n",
    "        for line in tqdm(file, total=n_nodes, desc=\"Nodes\"):\n",
    "            if line.startswith(\"*\"):\n",
    "                break\n",
    "            else:\n",
    "                parts = line.split(\"\\\"\")\n",
    "                node_id = parts[0].strip()\n",
    "                doi = parts[1].strip()\n",
    "                year = int(parts[2].strip())\n",
    "                \n",
    "                # Extract journal from DOI: \"10.1203/PhysRevA.81.012102\" -> \"PhysRevA\")\n",
    "                journal = doi.split(\"/\")[1].split(\".\")[0] if \"/\" in doi else \"Unknown\"\n",
    "                \n",
    "                G.add_node(node_id, doi=doi, year=year, journal=journal)\n",
    "        \n",
    "        # Read edges\n",
    "        edges = []\n",
    "        for line in tqdm(file, desc=\"Edges\"):\n",
    "            edge = line.split()[:2]\n",
    "            if len(edge) == 2:\n",
    "                edges.append(tuple(edge))\n",
    "        \n",
    "        G.add_edges_from(edges)\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = read_pajek_citations(\"data/aps_citations.net\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0f236",
   "metadata": {},
   "source": [
    "200592 represent the lines parsed not edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9a43e",
   "metadata": {},
   "source": [
    "Lets verify network statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a9488a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 56,473\n",
      "Edges: 200,353\n"
     ]
    }
   ],
   "source": [
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges()\n",
    "\n",
    "print(f\"Nodes: {n_nodes:,}\")\n",
    "print(f\"Edges: {n_edges:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d80f69",
   "metadata": {},
   "source": [
    "Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46cc92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_nodes == 56473, f\"Expected 56,473 nodes but got {n_nodes:,}\"\n",
    "assert n_edges == 200353, f\"Expected 200,353 edges but got {n_edges:,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6429be7",
   "metadata": {},
   "source": [
    "Year distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96a2605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2010: 12,680 papers\n",
      " 2011: 13,862 papers\n",
      " 2012: 15,268 papers\n",
      " 2013: 14,663 papers\n"
     ]
    }
   ],
   "source": [
    "year_counts = Counter(G.nodes[n]['year'] for n in G.nodes())\n",
    "for year in sorted(year_counts.keys()):\n",
    "    print(f\" {year}: {year_counts[year]:,} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f482bfa",
   "metadata": {},
   "source": [
    "The dataset almost balanced across years with a gradual increase in the number of papers from 2010 to 2012 and a slight drop in 2013.\n",
    "This means that the if we do temporal split (training on 2010–2012 and testing on 2013) it will provides a reasonable and representative division for model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149bda87",
   "metadata": {},
   "source": [
    "Journal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9b24825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique journals: 9\n",
      "  PhysRevA: 8,310 papers\n",
      "  PhysRevB: 16,060 papers\n",
      "  PhysRevC: 3,157 papers\n",
      "  PhysRevD: 10,461 papers\n",
      "  PhysRevE: 5,653 papers\n",
      "  PhysRevLett: 12,116 papers\n",
      "  PhysRevSTAB: 348 papers\n",
      "  PhysRevX: 179 papers\n",
      "  RevModPhys: 189 papers\n"
     ]
    }
   ],
   "source": [
    "journal_counts = Counter(G.nodes[n]['journal'] for n in G.nodes())\n",
    "print(f\"\\nNumber of unique journals: {len(journal_counts)}\")\n",
    "for journal, count in journal_counts.items():\n",
    "    print(f\"  {journal}: {count:,} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e261ecfc",
   "metadata": {},
   "source": [
    "Train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de81e7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set (2010-2012): 41,810 papers\n",
      "Test set (2013): 14,663 papers\n"
     ]
    }
   ],
   "source": [
    "train_nodes = [n for n in G.nodes() if G.nodes[n]['year'] <= 2012]\n",
    "test_nodes = [n for n in G.nodes() if G.nodes[n]['year'] == 2013]\n",
    "print(f\"\\nTrain set (2010-2012): {len(train_nodes):,} papers\")\n",
    "print(f\"Test set (2013): {len(test_nodes):,} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a3cc4",
   "metadata": {},
   "source": [
    "Lets now generate Node2Vec embedding. I will keep higher dimensions to capture more info. I will keep p=q=1 (mix BFS abd DFS) because it works well for citation networks. I will also keep more walks and longer walks to improve embedding quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 128  # embedding dimension (higher for large network)\n",
    "r = 30   # number of walks per node\n",
    "l = 80   # walk length\n",
    "p = 1.0  # return parameter (balanced)\n",
    "q = 1.0  # in-out parameter (balanced)\n",
    "window = 10  # context window for Word2Vec\n",
    "min_count = 1  # minimum word count\n",
    "workers = 8  # parallel workers\n",
    "\n",
    "node2vec = Node2Vec(\n",
    "    G, \n",
    "    dimensions=d, \n",
    "    walk_length=l, \n",
    "    num_walks=r, \n",
    "    p=p, \n",
    "    q=q,\n",
    "    workers=workers,\n",
    "    quiet=False  # Show progress\n",
    ")\n",
    "\n",
    "model = node2vec.fit(window=window, min_count=min_count, batch_words=4)\n",
    "wv = model.wv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e55de",
   "metadata": {},
   "source": [
    "Save embeddings and metadata and test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3683f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {node: wv[node] for node in G.nodes()}\n",
    "with open('data/aps_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "\n",
    "metadata = {\n",
    "    node: {\n",
    "        'journal': G.nodes[node]['journal'],\n",
    "        'year': G.nodes[node]['year'],\n",
    "        'doi': G.nodes[node]['doi']\n",
    "    }\n",
    "    for node in G.nodes()\n",
    "}\n",
    "with open('data/aps_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "\n",
    "split_info = {\n",
    "    'train_nodes': train_nodes,\n",
    "    'test_nodes': test_nodes\n",
    "}\n",
    "with open('data/aps_train_test_split.pkl', 'wb') as f:\n",
    "    pickle.dump(split_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "571e0f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training papers (2010-2012): 41,810\n",
      "Test papers (2013): 14,663\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training papers (2010-2012): {len(train_nodes):,}\")\n",
    "print(f\"Test papers (2013): {len(test_nodes):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264dc6b1",
   "metadata": {},
   "source": [
    "Lets again reload saved embeddings and metadata and test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43ee6111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train nodes: 41,810\n",
      "  Test nodes: 14,663\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open('data/aps_embeddings.pkl', 'rb') as f:\n",
    "    emb = pickle.load(f)\n",
    "with open('data/aps_metadata.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "with open('data/aps_train_test_split.pkl', 'rb') as f:\n",
    "    split = pickle.load(f)\n",
    "\n",
    "train_nodes = split['train_nodes']\n",
    "test_nodes  = split['test_nodes']\n",
    "\n",
    "print(f\"  Train nodes: {len(train_nodes):,}\")\n",
    "print(f\"  Test nodes: {len(test_nodes):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ad5ab",
   "metadata": {},
   "source": [
    "All good so far. I loaded the saved embeddings and metadata and asserted. I will train on 2010 to 2012 and test on 2013. For a light and defensible tuning step, I will select the Logistic Regression regularization C by validating on 2012 and training on 2010 to 2011. I will also train Random Forest and Linear SVM as baselines.Then I will report accuracy, macro F1 and for Logistic Regression also top 3 accuracy.I will also show a confusion matrix with a consistent label order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c857cf7",
   "metadata": {},
   "source": [
    "Pack features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8951cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (41810, 128) Test: (14663, 128)\n",
      "Train journals: 9 Test journals: 9\n"
     ]
    }
   ],
   "source": [
    "def pack(nodes):\n",
    "    X, y, yrs = [], [], []\n",
    "    for n in nodes:\n",
    "        if n in emb and n in meta:\n",
    "            X.append(emb[n])\n",
    "            y.append(meta[n]['journal'])\n",
    "            yrs.append(meta[n]['year'])\n",
    "    return np.vstack(X), np.array(y), np.array(yrs)\n",
    "\n",
    "X_train_full, y_train_full, years_train_full = pack(train_nodes)  # 2010–2012\n",
    "X_test,        y_test,        years_test     = pack(test_nodes)   # 2013\n",
    "\n",
    "print(\"Train:\", X_train_full.shape, \"Test:\", X_test.shape)\n",
    "print(\"Train journals:\", len(set(y_train_full)), \"Test journals:\", len(set(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ce674",
   "metadata": {},
   "source": [
    "Train and test sets have equal journal coverage and consistent 128-Dim embeddings, ensuring balanced representation and structural continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14c5a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen-label coverage in 2013: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check of Label Coverage\n",
    "seen = set(y_train_full)\n",
    "mask_seen = np.isin(y_test, list(seen))\n",
    "print(f\"Seen-label coverage in 2013: {mask_seen.mean():.3f}\")\n",
    "X_test_seen, y_test_seen = X_test[mask_seen], y_test[mask_seen]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afabe644",
   "metadata": {},
   "source": [
    "All 2013 papers belong to journals already present in the training set so seen-label coverage is 1. This will make sure that evaluation will be fair and the model is not tested on unseen classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3697c",
   "metadata": {},
   "source": [
    "Lets now do temporal hyperparameter tuning for logistic regression using 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffab23fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression params: {'C': 0.5}\n"
     ]
    }
   ],
   "source": [
    "is_val = (years_train_full == 2012)\n",
    "ps = PredefinedSplit(test_fold=np.where(is_val, 0, -1))\n",
    "lr_base = LogisticRegression(max_iter=1000, multi_class=\"multinomial\",\n",
    "                             class_weight=\"balanced\", solver=\"lbfgs\", random_state=42)\n",
    "grid = GridSearchCV(lr_base, {\"C\": [0.5, 1.0, 2.0]},\n",
    "                    scoring=\"f1_macro\", cv=ps, n_jobs=-1, refit=True)\n",
    "grid.fit(X_train_full, y_train_full)\n",
    "clf_lr = grid.best_estimator_\n",
    "print(\"Best Logistic Regression params:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c5e4c",
   "metadata": {},
   "source": [
    "Smaller C means stronger regularization. This shows that (C = 0.5) a slightly stronger penalty on large weights gives better generalization for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfce876",
   "metadata": {},
   "source": [
    "Lets build RF and SVM models with no heavy hyperparameter tuning to keep it light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7235610",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf  = RandomForestClassifier(n_estimators=300, class_weight=\"balanced_subsample\",\n",
    "                                 random_state=42, n_jobs=-1).fit(X_train_full, y_train_full)\n",
    "clf_svm = LinearSVC(max_iter=2000, random_state=42).fit(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1585b70",
   "metadata": {},
   "source": [
    "Evaluation helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c59d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(name, clf, X, y, topk=False):\n",
    "    yhat = clf.predict(X)\n",
    "    acc = accuracy_score(y, yhat)\n",
    "    f1m = f1_score(y, yhat, average=\"macro\", zero_division=0)\n",
    "    out = {\"acc\": acc, \"f1_macro\": f1m, \"yhat\": yhat}\n",
    "    if topk and hasattr(clf, \"predict_proba\"):\n",
    "        proba = clf.predict_proba(X)\n",
    "        out[\"top3\"] = top_k_accuracy_score(y, proba, k=3)\n",
    "    print(f\"{name:18s}  Acc={acc:.4f}  MacroF1={f1m:.4f}\" + (f\"  Top3={out['top3']:.4f}\" if \"top3\" in out else \"\"))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de96f8e",
   "metadata": {},
   "source": [
    "Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fd4c320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression  Acc=0.5797  MacroF1=0.4860  Top3=0.9168\n",
      "RandomForest        Acc=0.7166  MacroF1=0.5514\n",
      "LinearSVM           Acc=0.6988  MacroF1=0.5213\n"
     ]
    }
   ],
   "source": [
    "res_lr_seen  = eval_model(\"LogisticRegression\", clf_lr, X_test_seen, y_test_seen, topk=True)\n",
    "res_rf_seen  = eval_model(\"RandomForest\",       clf_rf, X_test_seen, y_test_seen)\n",
    "res_svm_seen = eval_model(\"LinearSVM\",          clf_svm, X_test_seen, y_test_seen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90503aa1",
   "metadata": {},
   "source": [
    "top-3 = 0.9168 shows that if the LR classifier can name three possible journals for each paper, the correct journal is in that list about 92% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd325a87",
   "metadata": {},
   "source": [
    "Lets now choose best by Macro-F1 on seen test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f5409fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model on seen test: RF\n"
     ]
    }
   ],
   "source": [
    "candidates = {\"LR\": res_lr_seen, \"RF\": res_rf_seen, \"SVM\": res_svm_seen}\n",
    "best_name = max(candidates, key=lambda k: candidates[k][\"f1_macro\"])\n",
    "best_pred = candidates[best_name][\"yhat\"]\n",
    "print(f\"\\nBest model on seen test: {best_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f039ee28",
   "metadata": {},
   "source": [
    "Insights: RF is best at first-try correctness (about 72% accuracy, macro-F1 ≈ 0.55). While LR is not as good on the very first guess but it is great at shortlisting the right journal (top-3 ≈ 92%). Linear SVM is close to Random Forest but a bit behind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd708c2c",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f4f1241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    PhysRevA       0.71      0.79      0.75      2264\n",
      "    PhysRevB       0.71      0.92      0.80      3864\n",
      "    PhysRevC       0.83      0.83      0.83       845\n",
      "    PhysRevD       0.83      0.96      0.89      2761\n",
      "    PhysRevE       0.76      0.66      0.71      1589\n",
      " PhysRevLett       0.42      0.22      0.29      3124\n",
      " PhysRevSTAB       0.70      0.69      0.69        90\n",
      "    PhysRevX       0.00      0.00      0.00        84\n",
      "  RevModPhys       0.00      0.00      0.00        42\n",
      "\n",
      "    accuracy                           0.72     14663\n",
      "   macro avg       0.55      0.56      0.55     14663\n",
      "weighted avg       0.68      0.72      0.68     14663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_seen, best_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96207825",
   "metadata": {},
   "source": [
    "Commentary: Test model on the 2013 test set was Random Forest with accuracy around 0.72 and macro F1 around 0.55. Logistic Regression produced a strong top 3 accuracy near 0.92 which shows that the correct journal is often among the top three suggestions even when the top one is incorrect. Performance is strong for PhysRevB and PhysRevD and reasonable for PhysRevA and PhysRevC. PhysRevLett and PhysRevX remain challenging due to class imbalance and overlap in citation neighborhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7770fa",
   "metadata": {},
   "source": [
    "Lets save final resuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5417e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = {\n",
    "\"best_model_seen\": best_name,\n",
    "\"lr_params\": grid.best_params_,\n",
    "\"seen_coverage\": float(mask_seen.mean()),\n",
    "\"seen_metrics\": {k: {\"acc\": v[\"acc\"], \"f1_macro\": v[\"f1_macro\"], **({\"top3\": v[\"top3\"]} if \"top3\" in v else {})}\n",
    "for k, v in candidates.items()}\n",
    "}\n",
    "with open(\"data/aps_classification_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
